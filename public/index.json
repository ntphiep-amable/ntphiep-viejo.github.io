[{"content":"","date":"2023-11-22","permalink":"/tags/dagster/","section":"Tags","summary":"","title":"dagster"},{"content":"\rLời mở đầu # Bài viết đầu tiên của mình trên này, chủ yếu là dịch lại và thêm thắt chút từ hiểu biết, mong mọi người ủng hộ nhe 🥲😁\nI. Dagster là gì? # Dagster là một công cụ mã nguồn mở hỗ trợ Orchestrate Task (quản lý, tổ chức, điều phối và kiểm soát các tác vụ và công việc) . Nếu các bạn là Data Engineer hẳn sẽ phải biết Apache Airflow, về cơ bản mục đích sủ dụng của Dagster cũng giống như Airflow vậy (tuy nhiên chúng có nhiều điểm khác nhau, có thể mình sẽ viết một bài So sánh Airflow và Dagster). Nói chung, các công cụ trên hỗ trợ xây dựng data pipeline.\nDagster được xây dựng và phát triển bởi elementl. Trang web chính thức của Dagster: https://dagster.io/\nII. Ứng dụng và đặc điểm của Dagster # 1. Ứng dụng # Dagster là một công cụ mã nguồn mở, tập trung hỗ trợ Task Orchestrator. Dagster được thiết kế để phát triển và duy trì các data assets (ví dụ như Dataframe tables, các data sets, ML models, \u0026hellip;) Các data assets được sử dụng thông qua các function. Bạn khai báo các function và các data assets mà các fucntion đó tạo ra hoặc cập nhật. Sau đó ban sử dụng Dagster để chạy các fucntion đúng thời điểm và đảm bảo các assets luôn được cập nhật.\nDagster được sử dụng trong hầu như mọi giai đoạn của của vòng đời data development như local development, unit tests, integration tests, staging environments cho đến giai đoạn đưa lên production.\n2. Đặc điểm # Dagster sử dụng Python để hoạt động và có thể được cài đặt dễ dàng với Python pip (lưu ý Python version 3.8 trở lên). Để cài đặt Dagster cơ bản, chạy lênh sau trong terminal:\npip install dagster dagster-webserver Trong bài này chúng ta sẽ build một project đơn giản. Chúng ta collect dữ liệu từ một trang tổng hợp tin tức (ở đây là Hacker News), làm sạch nó và build một report đơn giản. Ta sẽ dùng Dagster để cập nhật data và report định kì, cái mà Dagster gọi là assets.\nTrước hết bạn cần hiểu về một khái niệm cốt lõi của Dagster, đó là Software-defined asset (SDA) . Một asset là một \u0026hellip; đối tượng trong bộ lưu trữ liên tục, cái mà có thể ghi lại một \u0026hellip; cái gì đó 😀😄 (lú vcl ~~ ). Cơ bản thì asset có thể là:\nMột table hoặc view trong database Một file (như file trên máy của bạn 😀) Một model Machine Learning Nếu bạn đã có một datap pipeline, bạn đã có những assets. Software-defined assets là một khái niệm cho bạn ghi data pipeline dựa trên các asset.\nDưới đây là một ví dụ. Một asset là một dataset tên là topstories, nó phụ thuộc vào một asset khác tên là topstory_ids. topstories lấy ID được tính toán trong topstory_ids, sau đó fetch data cho từng ID đó.\n@asset(deps=[topstory_ids]) def topstories() -\u0026gt; None: with open(\u0026#34;data/topstory_ids.json\u0026#34;, \u0026#34;r\u0026#34;) as f: topstory_ids = json.load(f) results = [] for item_id in topstory_ids: item = requests.get( f\u0026#34;https://hacker-news.firebaseio.com/v0/item/{item_id}.json\u0026#34; ).json() results.append(item) if len(results) % 20 == 0: print(f\u0026#34;Got {len(results)} items so far.\u0026#34;) df = pd.DataFrame(results) df.to_csv(\u0026#34;data/topstories.csv\u0026#34;) Một tập hợp các asset sẽ có dạng một đồ thị DAG (directed acyclic graph) (ai sử dụng Airflow sẽ phải biết cái này). Mỗi cạnh trong DAG tương ứng với sự phụ thuộc data giữa các asset. DAG giúp bạn:\nHiểu về sự liên quan giữa các asset. Giúp làm việc với data pipeline dễ dàng, hiệu quả hơn. Dagster có hỗ trợ giao diện người dùng (UI) dưới dạng web. Dưới dây là một DAG được biểu diễn trong User Interface: Lời kết # Trên đây là một số điều có bản các bạn cần biết về Dagster, chưa dài lắm cơ mà ngại viết 🤣, còn rất nhiều thứ về Dagster nhưng mình sẽ viết ở những bài sau (hoặc là không :)))). Bài tiếp theo có thể là một Sample project như đã nói ở trên nhé. Cảm ơn mọi người đã đọc!\nReferences # https://docs.dagster.io/getting-started\n","date":"2023-11-22","permalink":"/posts/dagster_1/","section":"Posts","summary":"Lời mở đầu # Bài viết đầu tiên của mình trên này, chủ yếu là dịch lại và thêm thắt chút từ hiểu biết, mong mọi người ủng hộ nhe 🥲😁","title":"Dagster Là Gì? Dagster Cơ Bản Cho Người Mới Bắt Đầu"},{"content":"","date":"2023-11-22","permalink":"/tags/data/","section":"Tags","summary":"","title":"data"},{"content":"","date":"2023-11-22","permalink":"/","section":"Hiep's blog","summary":"","title":"Hiep's blog"},{"content":"","date":"2023-11-22","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"2023-11-22","permalink":"/tags/python/","section":"Tags","summary":"","title":"python"},{"content":"","date":"2023-11-22","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"\r¡Hola a todos, soy Hiep! # Este es mi blog personal, aquí escribiré sobre temas como vida, programación, opiniones, emociones o temas varios\u0026hellip;\nGracias por leer😙.\nLói chung là cái cuộc sống này buồn ~\n¡Noviembre 2023, Hiep! # ","date":"2023-11-21","permalink":"/info/","section":"Hiep's blog","summary":"¡Hola a todos, soy Hiep! # Este es mi blog personal, aquí escribiré sobre temas como vida, programación, opiniones, emociones o temas varios\u0026hellip;\nGracias por leer😙.\nLói chung là cái cuộc sống này buồn ~","title":"About NTPH"},{"content":"","date":"2023-11-21","permalink":"/tags/shop/","section":"Tags","summary":"","title":"shop"},{"content":"\rThis is my shopp # ","date":"2023-11-21","permalink":"/shop/","section":"Hiep's blog","summary":"\rThis is my shopp # ","title":"Shop"},{"content":"\rLời mở đầu # Ở bài trước - Giới thiệu DBT - mình đã đề cập đến việc dùng dbt kết nối và làm việc với các data platform khác. Hôm nay mình sẽ kết nối đến và thực hiện một vài thao tác cơ bản đến Google Big Query - một trong những Data platform phổ biến nhất thời điểm hiện tại.\nTrong bài, mình sẽ dùng một dataset mẫu để mô phỏng quá trình. Dataset tên là jaffle_shop. Nếu bạn thực hành với dataset khác, thao tác tương tự 🥲.\nCác setup cần thiết # Để có thể sử dụng một công cụ nào đó, đương nhiên là trước tiên bạn phải cài đặt rồi 😄\nVới dbt, mình đã hướng dẫn cài dbt ở bài trước, dùng pip đơn giản: pip install dbt-core dbt-bigquery Sau đó, kiểm tra lại với lệnh:\ndbt --version Với Big query, mình sẽ nói ngắn gọn các thao tác cần thiết, chi tiết các bạn có thể tìm đọc các trang hướng dẫn Big Query cơ bản\u0026hellip; Trước hết, bạn cần phải có tài khoản Google trước, sau đó truy cập BigQuery Console, tạo một project mới đặt tên gì cũng được. Sau đó, hãy thử với câu query cơ bản sau khi tạo project: select * from `dbt-tutorial.jaffle_shop.customers`; Nếu không có gì sai thì result sẽ như sau: Sau khi thử chạy thành công trên console của Big query, hãy tạo một dataset mới (\rhướng dẫn), ở đây mình đặt là jaffle_shop. Ai đã làm việc với Big query sẽ biết dataset tương đương với database hoặc schema trong các hệ cơ sở dữ liệu cơ bản. Các bước tiếp theo là Tạo Credential Trong Big query, cuối cùng là download keyfile dạng JSON về. Keyfile JSON có format như sau:\n{ \u0026#34;type\u0026#34;: \u0026#34;service_account\u0026#34;, \u0026#34;project_id\u0026#34;: \u0026#34;PROJECT_ID\u0026#34;, \u0026#34;private_key_id\u0026#34;: \u0026#34;KEY_ID\u0026#34;, \u0026#34;private_key\u0026#34;: \u0026#34;-----BEGIN PRIVATE KEY-----\\nPRIVATE_KEY\\n-----END PRIVATE KEY-----\\n\u0026#34;, \u0026#34;client_email\u0026#34;: \u0026#34;SERVICE_ACCOUNT_EMAIL\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;CLIENT_ID\u0026#34;, \u0026#34;auth_uri\u0026#34;: \u0026#34;https://accounts.google.com/o/oauth2/auth\u0026#34;, \u0026#34;token_uri\u0026#34;: \u0026#34;https://accounts.google.com/o/oauth2/token\u0026#34;, \u0026#34;auth_provider_x509_cert_url\u0026#34;: \u0026#34;https://www.googleapis.com/oauth2/v1/certs\u0026#34;, \u0026#34;client_x509_cert_url\u0026#34;: \u0026#34;https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\u0026#34; } Vậy là OK phần setup chuẩn bị các thứ rồi 😁, giờ hãy đến phần chính nào!\nKết nối dbt đến Big query # Tạo project với dbt # Di chuyển đến thư mục bạn muốn tạo project, chạy lệnh sau:\ndbt init jaffle_shop Như đã nói, mình sẽ lấy tên project là jaffle_sshop, bạn có thể đặt tên khác tùy ý. Hãy mở project vừa tạo lên, ở đây mình sử dụng VS-Code. Bạn sẽ thấy những file và folder được tạo trong thư mục project, mỗi file và folder đảm nhận chức năng khác nhau và cần thiết cho project.\nÝ nghĩa các file và folder trong thư mục project # analyses: thư mục hỗ trợ tổ chức các truy vấn SQL phân tích trong dự án, chứa các script sql mà analytic cần, cơ mà nó chỉ compile chứ không có excute. macros: chứa các block code có thể tái sử dụng nhiều lần, tiện lợi đỡ phí thời gian, viết 1 lần rồi thích gọi ở đâu cũng được. models: là thư mục quan trọng nhất của project, đơn giản bởi vì nó là folder chứa các model :))). Mỗi file .sql trong folder này là một model (có thể là table, view, \u0026hellip;). Khi chạy dbt run sẽ tự generate model và insert data. seeds: chứa các file .csv, static data, giúp load data từ file vào các data platform thuận tiện hơn. snapshots: giúp chứa snapshot data của một table tại một thời điểm nào đó nếu dữ liệu các bảng có thể bị thay đổi. tests: đơn giản như cái tên 😀, chứa các query bạn dùng để test các model và resource. Khi chạy dbt test sẽ giúp bạn test tự động. dbt_project.yml: file config quan trọng không thể thiếu của project, chứa các thông tin như name, version, profile, path, vars, \u0026hellip; cần thiết cho chạy dự án. Tạm thời khi mới khởi tạo project thì chỉ có vậy, nhưng sau khi bạn thực hiện kết nối, sẽ có thêm vài file và folder sinh ra thêm như seed, logs, target, \u0026hellip; Cơ mà lười giải thích nên để sau nhé 😌\nThực hiện kết nối # DBT sẽ thực hiện kết nối đến Data warehouse sử dụng profile. Nó được định nghĩa trong một file gọi là profiles.yml, trong đó chứa tất cả các thông tin chi tiết cần thiết để thực hiện kết nối đến data warehouse. Ở thư mục ~/.dbt (nếu là linux) hoặc C:\\Users\\Username\\.dbt (nếu là windows), bạn hãy tạo file profiles.yml. Sau đó, hãy copy file JSON chứa key đã tải ở phần trên vào cùng thư mục này, tiếp theo hãy copy và sửa các giá trị cho phù hợp.\njaffle_shop: # this needs to match the profile in your dbt_project.yml file target: dev outputs: dev: type: bigquery method: service-account keyfile: # replace this with the full path to your keyfile project: # Replace this with your project id dataset: # Replace this with dbt_your_name threads: 1 timeout_seconds: 300 location: US priority: interactive Note: Tại sao lại để profiles.yml ngoài thư mục project? Đó là bởi vì lý do hạn chế các sensitive credentials(các thông tin nhạy cảm) bị check bởi các công cụ version control. Thực ra thì bạn có thể để chung với thư mục project cũng được với điều kiện là nên sử dụng các environment variables (biến môi trường) để load các sensitive credentials. Còn nếu để ngoài thì dbt sẽ tự động tìm trong thư mục ~/.dbt. Ô cê, cuối cùng hãy vào thư mục chứa project và chạy:\ndbt debug Và nếu kết quả hiển thị như lày là bạn đã kết nối thành công rồi!!\nConnection test: OK connection ok Vậy là thành công kết nối dbt đến Big Query rồi, các bạn có thể thực hiện các công việc như build model, load data, chạy test, schedule jobs, \u0026hellip;Có thể mình sẽ hướng dẫn ở những bài tiếp theo. Rất đơn giản phải khum 😉. Chúc các bạn kết nối thành công nhé!\nReference # https://docs.getdbt.com/quickstarts/manual-install?step=1\nhttps://docs.getdbt.com/quickstarts/bigquery?step=1\n","date":"2023-09-25","permalink":"/posts/dbt_2/","section":"Posts","summary":"Lời mở đầu # Ở bài trước - Giới thiệu DBT - mình đã đề cập đến việc dùng dbt kết nối và làm việc với các data platform khác.","title":"[DBT] Kết Nối DBT Với Big Query"},{"content":"","date":"2023-09-25","permalink":"/tags/bigquerry/","section":"Tags","summary":"","title":"bigquerry"},{"content":"","date":"2023-09-25","permalink":"/tags/dbt/","section":"Tags","summary":"","title":"dbt"},{"content":"\rLời mở đầu # Bài này là mình dịch và sửa lại từ một bài viết thấy khá hay, chi tiết và đầy đủ. Link bài viết mình để ở dưới nhé!. Chúc mọi người một ngày vui vẻ 😁.\nData Warehouse?? # Data Warehouse (Kho dữ liệu) là một hệ thống quản lý dữ liệu được sử dụng để lưu trữ và tính toán dữ liệu, cho phép thực hiện các hoạt động phân tích như chuyển đổi (transforming) và chia sẻ (sharing) dữ liệu. Nó giúp doanh nghiệp nắm bắt và lưu trữ dữ liệu từ các nguồn bên ngoài. Các kỹ sư phân tích và nhà phân tích dữ liệu sử dụng nó để truy vấn các tập dữ liệu bằng SQL, biến chúng thành các mô hình (models) và báo cáo dữ liệu mạnh mẽ. Data warehouse là nguồn trung tâm cho bất kỳ ngăn xếp dữ liệu hiện đại nào. Dữ liệu được nhập, chuyển đổi và chia sẻ (imported, transformed, and shared) với các công cụ khác từ kho.\nHiện tại, có 2 loại data warehouse chính: On-prem (tại chỗ) và Cloud (nền tảng đám mây). On-prem warehouse là một vị trí thực tế nơi các công ty cần duy trì phần cứng và phần mềm để lưu trữ dữ liệu. Trong khi đó, cloud warehouse có sẵn ở mọi nơi và không bao gồm vị trí thực tế bạn cần truy cập, tuy nhiên, bạn sẽ phải trả tiền để sử dụng không gian lưu trữ và sức mạnh tính toán do một công ty thứ 3 khác cung cấp và duy trì. Có thể kể đến như AWS (Amazon Web Services), GCP (Google Cloud Platform), \u0026hellip;\nNguồn gốc của Data warehouse # Mặc dù dữ liệu đã được lưu trữ trong suốt lịch sử nhưng phải đến những năm 1980, công nghệ mới bắt đầu tăng tốc và Data warehouse thức đầu tiên được tạo ra. Đó là một on-prem warehouse bao gồm rất nhiều storage towers (tháp lưu trữ) và các vi xử lý máy tính, chiếm rất nhiều không gian. Và như bạn có thể tưởng tượng, điều này gây ra rất nhiều vấn đề. Nó không chỉ chiếm nhiều không gian vật lý mà nhân viên còn phải bảo trì phần cứng và phần mềm của nhưng thiết bị cấu hình warehouse này. Điều này nhanh chóng trở nên tốn kém và không thực tế đối với các công ty nhỏ hơn không có ngân sách hoặc không gian.\nKhi Amazon bắt đầu mở rộng quy mô kho dữ liệu tại chỗ để hỗ trợ hoạt động kinh doanh của mình, họ nhận thấy cơ hội bán năng lực tính toán cho các doanh nghiệp khác để tiết kiệm chi phí :))). Đây là lúc Redshift, sản phẩm cloud data warehouse củaAmazon, ra đời. Ngay sau đó, những gã khổng lồ công nghệ khác như Google và Microsoft cũng đang xây dựng cơ sở hạ tầng dữ liệu cũng làm theo.\nGiờ đây, bạn có thể tiếp cận và sử dụng sức mạnh của cloud warehouse ở bất cứ đâu. Bạn không cần phải tự mình duy trì cơ sở hạ tầng nữa mà có thể trả tiền cho một công ty để làm việc này cho bạn. Điều này rẻ hơn so với on-prem phải trả để duy trì hệ thống và cho phép khả năng dữ liệu nhanh hơn.\nTại sao các doang nghiệp cần phải có Data warehouse # Data warehouse đã từng được cho không thực tế do chi phí liên quan đến chúng. Giờ đây, kho lưu trữ đám mây cung cấp chúng cho gần như tất cả mọi người, chúng mang lại rất nhiều lợi ích cho doanh nghiệp. Kho đám mây cho phép khả năng mở rộng, tính sẵn có, tiết kiệm chi phí và tăng cường bảo mật - tất cả đều do chính nhà cung cấp xử lý. Các tiện ích có thể được liệt kê:\nScalability (Khả năng mở rộng): Data warehouse cho phép bạn mở rộng quy mô tính toán lên hoặc xuống tùy thuộc vào tốc độ bạn cần chạy các phép biến đổi của mình và số tiền bạn sẵn sàng chi tiêu. Bạn cũng có thể bật hoặc tắt tài nguyên máy tính để tiết kiệm chi phí Availability (Sẵn có): Data warehouse luôn có sẵn. Mặc dù độ trễ có thể thay đổi tùy theo vị trí nguồn và đích nhưng dữ liệu của bạn có thể được truy cập ở mọi nơi, mọi lúc, rất tiện lợi. Điều này trở nên lý tưởng trong xã hội hiện đại, nơi mọi người có thể làm việc từ bất cứ đâu. Cost savings (Tiết kiệm chi phí): So với on-prem warehouse, cloud warehouse tiết kiệm hơn nhiều vì bạn không còn cần phải bảo trì tất cả cơ sở hạ tầng nên bạn có thể tiết kiệm chi phí liên quan đến bảo trì. Các công ty kho dữ liệu quản lý rất nhiều dữ liệu nên họ có thể tiết kiệm chi phí mà bạn không thể làm được. Security (Tính bảo mật): Data warehouse cung cấp các tính năng bảo mật nâng cao để đảm bảo dữ liệu của bạn luôn được bảo mật. Nó thường trực tiếp xử lý các chiến lược tuân thủ nhất định cần thiết đối với từng loại dữ liệu khác nhau , giúp bạn không cần phải tự mình thực hiện việc này. Nó cũng có các tính năng như vai trò và người dùng giúp bạn kiểm soát ai có quyền truy cập vào dữ liệu của mình. Nhưng chúng ta sẽ đi sâu vào vấn đề này sau. Tiềm năng trong ứng dụng doanh nghiệp # Các doanh nghiệp có thể tận dụng kho dữ liệu vì nhiều lý do khác nhau. Hầu hết những lý do này đều giúp tiết kiệm thời gian và tiền bạc cho doanh nghiệp, dù trực tiếp hay gián tiếp.\nHợp nhất tất cả dữ liệu về một nơi # Thay vì để tất cả dữ liệu của bạn trải rộng trên các nền tảng khác nhau, dữ liệu đó có sẵn cho bạn ở một nơi. Điều này cho phép bạn chuẩn hóa tất cả các metrics (chỉ số cốt lõi) và definitions (định nghĩa dữ liệu) của mình, thay vì phụ thuộc vào các chỉ số được tính toán bởi các nền tảng như Google và Facebook. Nếu bạn thấy rằng các số liệu khác nhau không phù hợp trên các nền tảng thì data warehouse sẽ đóng vai trò là nguồn đáng tin cậy cho số liệu phù hợp. Thay vì dựa vào các nền tảng bên ngoài, giờ đây bạn đã có một nền tảng tập trung tất cả dữ liệu của mình.\nChưa kể, bạn sẽ khiến kỹ sư (DE) và nhà phân tích dữ liệu (DA) của mình phải đau đầu. Nếu không, họ sẽ phải lấy dữ liệu cần thiết từ nhiều nguồn khác nhau theo cách thủ công. Việc không có một nguồn thông tin chính xác duy nhất sẽ làm giảm chất lượng dữ liệu của bạn, lãng phí thời gian của nhóm dữ liệu và gây khó khăn cho việc kết hợp dữ liệu từ các nguồn khác nhau.\nKhả năng kiểm soát các quyền truy cập và loại quyền truy cập của các đối tượng # Data warehouse có các tính năng bảo mật mở rộng cho phép bạn kiểm soát ai có quyền truy cập vào nội dung gì. Bạn có khả năng cấp cho ai đó các quyền ít hoặc nhiều tùy theo ý muốn của bạn. Nó cũng cung cấp cho bạn khả năng tạo người dùng và gán vai trò cho họ. Mỗi vai trò có bộ quyền riêng đối với cơ sở dữ liệu và bảng mà nó có thể xem. Sau đó, bạn cũng có thể chọn người được phép thực hiện query (truy vấn) các bảng đó hoặc thậm chí update (cập nhật) và delete (xóa) chúng.\nKhi bất kỳ ai trong tổ chức của bạn có thể dễ dàng truy cập vào dữ liệu của bạn, điều tồi tệ có thể xảy ra. Nguy cơ dữ liệu quan trọng có thể bị xóa, chỉnh sửa sai hoặc truy cập không thích hợp. Người dùng, vai trò, chính sách và biện pháp bảo mật của kho dữ liệu có thể giúp đảm bảo dữ liệu nằm trong tay đúng người.\nThích hợp cho Fast reporting (báo cáo nhanh) # Vì tất cả dữ liệu của bạn đều nằm ở cùng một nơi nên nó cho phép báo cáo nhanh hơn so với việc lấy dữ liệu từ nhiều nguồn khác nhau. Vị trí trung tâm cho phép bạn truy cập và truy vấn nhanh chóng hàng triệu hàng dữ liệu, cho phép thực hiện chuyển đổi và báo cáo nhanh hơn nhiều.\nCác Data platform hỗ trợ data warehousing workloads # Hiện nay có nhiều các nền tảng cung cấp, hỗ trợ Data warehouse dưới dạng dịch vụ, có thể kể đến như:\nData platform Mô tả Snowflake Snowflake là một nền tảng được quản lý hoàn toàn để lưu trữ dữ liệu, hồ dữ liệu (data lake), kỹ thuật dữ liệu, khoa học dữ liệu và phát triển ứng dụng dữ liệu. Databricks Databricks là một nền tảng phân tích dữ liệu, kỹ thuật dữ liệu và khoa học dữ liệu cộng tác dựa trên đám mây, kết hợp những gì tốt nhất của data warehouse và data lake vào kiến trúc lakehouse 😀. Google BigQuery Google BigQuery là một serverless (không máy chủ) warehouse, có khả năng mở rộng cao, đi kèm với công cụ truy vấn tích hợp.. Amazon Redshift Amazon Redshift là data warehouse dựa trên đám mây có quy mô petabyte được quản lý hoàn toàn, được thiết kế để lưu trữ và phân tích tập dữ liệu quy mô lớn (biggg data). Postgres PostgreSQL là một cơ sở dữ liệu quan hệ mã nguồn mở cấp doanh nghiệp nâng cao hỗ trợ cả truy vấn SQL (quan hệ) và JSON (không quan hệ). Data warehouse vs Data lake # Data lake (hồ dữ liệu) là một hệ thống nơi bạn lưu trữ, xử lý và truy vấn dữ liệu phi cấu trúc, bán cấu trúc và có cấu trúc ở hầu hết mọi quy mô. Sự khác biệt chính giữa data warehouse và data lake là loại và cách lưu trữ dữ liệu. Data warehouse chứa dữ liệu có cấu trúc nhằm tổ chức dữ liệu để sử dụng phân tích, trong hi đó Data lake có thể chứa khá nhiều loại dữ liệu—có cấu trúc hoặc không cấu trúc—và dữ liệu thường được giữ nguyên ở định dạng thô cho đến khi sẵn sàng sử dụng.\nHiểu đơn giản thì data lake chứa đủ mọi loại mọi kiểu dữ liệu tùm lum, một phần dữ liệu có cấu trúc trong data lake sẽ được load vào data warehouse trong quá trình ETL hoặc ELT.\nKết luận # Data warehouse đã đi một chặng đường dài trong 40 năm qua. Nó bắt đầu như một bộ máy vật lý thực tế với chi phí khổng lồ đến một hệ thống có sẵn cho bất kỳ ai, ở bất kỳ đâu với chi phí phải chăng. Nó có khả năng tập trung tất cả dữ liệu của doanh nghiệp bạn, cho phép thực hiện các hoạt động phân tích nhanh hơn, KPI được tiêu chuẩn hóa và một nguồn thông tin đáng tin cậy duy nhất. Tất cả các doanh nghiệp đều cần một kho dữ liệu để hoạt động nhanh chóng và hiệu quả với dữ liệu mà họ có thể dựa vào. Câu hỏi không phải là bạn có cần data warehouse hay không mà là bạn nên chọn loại data warehouse nào.\nReference # https://docs.getdbt.com/terms/data-warehouse\n","date":"2023-09-24","permalink":"/posts/datawarehouse/","section":"Posts","summary":"Lời mở đầu # Bài này là mình dịch và sửa lại từ một bài viết thấy khá hay, chi tiết và đầy đủ. Link bài viết mình để ở dưới nhé!","title":"[Data Warehouse] Kiến Thức Tổng Quan Về Data Warehouse (Kho Dữ Liệu)"},{"content":"","date":"2023-09-24","permalink":"/tags/data-science/","section":"Tags","summary":"","title":"data science"},{"content":"","date":"2023-09-24","permalink":"/tags/datawarehouse/","section":"Tags","summary":"","title":"datawarehouse"},{"content":"\rLời mở đầu # Các bạn DE, DA khi tìm search từ khóa \u0026ldquo;dbt\u0026rdquo; trên Google thường sẽ ra nhiều kết quả khác nhau về những cụm từ có viết tắt là \u0026ldquo;dbt\u0026rdquo;, và không may thay là cái mà DE, DA cần lại thường không được suggest đầu tiên :))). Cái mình nhắc đến hôm nay là Data Build Tool, một công cụ mã nguồn mở hỗ trợ chủ yếu việc transform data. Dạo gần đây thấy nó nổi lên khá là mạnh và được sử dụng tương đối nhiều.\nI. DBT là gì? # Đối với các bạn DA, muốn xây dựng một BI Model, việc module hóa quá trình transform data là thực sự cần thiết. Có rất nhiều tool hỗ trợ transform mạnh mẽ có thể kể đến như Pandas (python), Apache Spark, R, Apache Nifi, \u0026hellip;.. Tuy nhiên để có thể dễ dàng sử dụng cho các bạn đã biết SQL thì dbt là một lựa chọn rất tốt vì nó hỗ trợ module hóa các câu lệnh SQL, vậy nên các bạn chỉ cần có nền tảng SQL có thể dễ dàng sử dụng.\nNhư đã lói, nó là một tool open-source được xây dựng và phát triển bởi RJMetrics vào năm 2016. Mục đích ban đầu và cũng là mục đích chính của dbt là transform data. Data build tool sủ dụng SQL dó đó quá trình transform trở nên nhanh và dễ dàng hơn.\nĐiều làm dbt trở nên đặc biệt là dbt có thể giúp một Analyst bình thường có thể thực hiện được những công việc cơ bản của Engineer (chủ yếu là transform - biến đổi dữ liệu). DBT giúp việc transform, document, test data trở nên dễ dàng hơn và có thể nhân rộng được. Thực hiện các điều trên cũng trở nên đơn giản hơn thông qua việc sử dụng các công cụ của dbt chứ bạn không cần phải set up hệ thống test và viết document tách biệt.\nNguồn: https://www.getdbt.com/product/what-is-dbt/\nII. Các điểm nổi bật của dbt # Lý do dbt được lựa chọn trong việc transform dữ liệu cho cả những người mới và trong cả các công ty là bởi dbt có rất nhiều tính năng hay ho. Dưới đây mình sẽ liệt kê một vài tính năng và công cụ rất tiện lợi mà dbt cung cấp:\n1. Rút gọn boilerplate (đoạn code mang tính hình thức) # Mỗi DBMS (hệ quản trị cơ sở dữ liệu) sẽ sủ dụng một loại SQL khác nhau. Ví dụ như Oracle sử dụng PL-SQL trong khi Microsoft SQL Server sử dụng T-SQL, \u0026hellip;. Cơ bản thì nó vẫn là SQL tuy nhiên mỗi loại lại có những có pháp, cách sủ dụng có phần khác nhau không nhiều thì ít, kiểu như có loại sẽ dùng LIMIT, có loại dùng TOP, có loại dùng kiểu STRING có loại lại là VARCHAR, db có gốc Java thì có kiểu DOUBLE còn mấy cái khác thì không v.v\u0026hellip;.\nCũng vì lý do này mà không mấy ai thích việc phải viết và define từng table, column hay insert data vào database trực tiếp bằng SQL ( thậm chí developer còn tránh làm điều này mà dùng ORM để thay thế 🙃). DBT sinh ra để một phần cải thiện vấn đề này. Nó giúp lược bỏ những đoạn code mang tính hình thức. Điều này có thể giúp bạn thao tác tốt hơn với các table với số lượng lớn và phụ thuộc vào nhau.\nVí dụ bạn có một table tên customer, bạn muốn tạo một bảng mới tên là person với một số cột từ bảng customer, với SQL bình thường thì phải làm như sau:\nDROP TABLE IF EXISTS person; CREATE TABLE person AS ( SELECT ID, FirstName, LastName, City, Address FROM customer; ) Còn với dbt, bạn chỉ cần viết:\nSELECT ID, FirstName, LastName, City, Address FROM customer Và lưu lại với file tên person.sql, đơn giản và nhanh gọn là bạn đẫ có table tên person, đầy đủ data y hệt như khi viết SQL bình thường. Ở một ví dụ nhỏ như thế này chúng ta có thể thấy sự khác biệt không nhiều, nhưng trên thực tế rất có ích với quy mô hàng trăm tables cùng 1 lúc và phụ thuộc vào nhau.\n2. Hỗ trợ đa dạng Data Platform - Data Warehouse # DBT hỗ trợ kết nối và làm việc cùng với rất đa dạng các Data Platform khác nhau. Dưới đây là một số những data platform phổ biến mà dbt có thể kết nối và làm việc.\nNguồn: https://docs.getdbt.com/docs/supported-data-platforms\nĐối với mỗi data platfom mà bạn muốn làm việc, cần cài đặt 2 thư viện là dbt-core và thư viện tương ứng với platform đó. Ví dụ nếu muốn làm việc với Google BigQuery thì cài dbt-bigquery nữa là được\npip install dbt-core dbt-bigquery 3. Models hóa abstraction và dependency # Các table trong một database có một abstract dependency ( Tạm gọi là sự lệ thuộc ảo ) với nhau. Khi chạy SQL thông qua dbt, thì các câu SQL sẽ được chạy theo tuần tự như dependency đã được khởi tạo trong dbt-models.\n4. Data Lineage và documentation # Thêm nữa, dbt còn cung cấp công cụ document, khi bạn tạo ra dependency, dbt sẽ tự động tạo ra các tài liệu để biểu thị sự phụ thuộc giữa các model. Bạn sẽ không cần phải tự ghi tài liệu mà vẫn có tài liệu để nắm rõ nguồn gốc và mối liên hệ của data. Khá là tiện lợi!!\nNgoài việc có đồ thị cho data lineage, dbt còn hỗ trợ tự động tạo tài liệu cho data. Bạn có thể ghi lại ý nghĩa của từng column, table do bạn tạo ra (tương tự Metadata đã nhắc tới trong Dagster, tránh trường hợp sau này quên mất chúng là gì.\n5. Hỗ trợ Jinja template # Jinja là một template engine rất nổi tiếng, ai làm Front-End web chắc hẳn đã nghe qua và sử dụng. Trong trường hợp của dbt, nó sẽ giúp bạn thu gọn những câu lệnh SQL rất dài thành ngắn lại. Ví dụ như sau đây:\nselect * from {{ ref(\u0026#39;my_first_dbt_model\u0026#39;) }} where id = 1 Ở đây thay vì các bạn phải ghi chi tiết từ database đến table \u0026hellip;. để kết nối tới table my_first_dbt_model thì các bạn chỉ cần ghi ngắn gọn lại như vậy và dbt sẽ xử lý đầy đủ.\n6. Test tự động # DBT có rất nhiều loại test tự động khác nhau được soạn sẵn, hoặc bạn có thể tự viết bằng dbt macros. Như vậy bạn có thể chuẩn hóa quy trình bằng cách yêu cầu tất cả models pass các test được đề ra mà không cần phải check thủ công từng table.\n7. Có nền tảng Cloud # DBT có 2 bản Cloud và CLI, bản Cloud có giao diện rất đẹp cùng nhiều tính năng tiện lợi hay ho hơn bản CLI nhiều, và đương nhiên là mất phí rồi :)))\nLời kết # Cơ bản thì không có một công cụ nào là hoàn hảo về mọi măt. DBT cũng chỉ là một lựa chọn trong số rất nhiều công cụ khác. Trong một dự án phải sử dụng kết hợp nhiều công cụ mới có thể tối uu điểm mạnh. Tuy nhên DBT cũng là một công cụ khá hay ho mà các bạn nên trai nghiệm vì một phần nó không khó để bắt đầu và cũng vì nó đang dần trở nên phổ biến thời gian gần đây. Bài viết mình tổng hợp từ vài nguồn và hiểu biết, thiếu sót mong mọi người góp ý. Cảm ơn mọi người đã đọc!\nReference # https://tuananalytic.com/dbt-data-build-tool-la-gi/\nhttps://www.getdbt.com/\nhttps://www.getdbt.com/product/what-is-dbt/\n","date":"2023-08-05","permalink":"/posts/dbt_1/","section":"Posts","summary":"Lời mở đầu # Các bạn DE, DA khi tìm search từ khóa \u0026ldquo;dbt\u0026rdquo; trên Google thường sẽ ra nhiều kết quả khác nhau về những cụm từ có viết tắt là \u0026ldquo;dbt\u0026rdquo;, và không may thay là cái mà DE, DA cần lại thường không được suggest đầu tiên :))).","title":"DBT (Data Build Tool) Là Gì? Những Thứ Cơ Bản Về DBT"},{"content":"","date":"2023-08-05","permalink":"/tags/etl/","section":"Tags","summary":"","title":"etl"},{"content":"\rLời mở đầu # Trong bài trước mình đã giới thiệu với mọi người về Dagster, nó là gì, dùng để làm gì. Trong bài này mình sẽ hướng dẫn các bạn làm một project cơ bản với Dagster, cụ thể là build một data pipeline đơn giản. Ai quên thì kệ 🤪!\nI. Cài đặt thư viện và các thứ cần thiết # Để cài Dagster, bạn cần phải cài đặt Python bản 3.8 trở lên, sau đó có thể cài Dagster rất dễ dàng thông qua pip. Mở termianl và chạy câu lệnh sau:\npip install dagster dagster-webserver dagit Vậy là bạn đã có thể cài Dagster với chỉ thao tác đơn giản. Bạn cũng có thể cài Dagster thông qua source của nó, cơ mà cái nào đơn giản thì làm :))\nII. Định nghĩa các asset # Trong bài trước, chúng ta đã tìm hiểu asset là gì, tuy nhiên với một cách khá là lý thuyết. Hôm nay mình sẽ cho các bạn thấy asset thực sự là gì, cách implement nó trong code và trực quan của nó trông ra sao.\nNhư đã nói ở bài trước, chúng ta sẽ tạo một project cơ bản để build một data pipeline với các giai đoạn:\nDownloads top 10 stories từ HackerNews Tiến hành lọc và chọn ra các field cần thiết Ghi kết quả thu được vào Pandas Dataframe rồi cuối cùng lưu xuống file .csv Mình đang sử dụng Ubuntu. Tạo một folder tên gì cũng được :)) ở đây mình đặt tên là hello-dagster, di chuyển vào folder và tạo một file Python, ở đây mình đặt là hello-dagster.py.\nmkdir hello-dagster cd hello-dagster touch hello-dagster.py Code trong file hello-dagster.py:\nimport json import requests import pandas as pd from dagster import AssetExecutionContext, MetadataValue, asset @asset def hackernews_top_story_ids(): \u0026#34;\u0026#34;\u0026#34; Get top 10 stories from the HackerNews top stories endpoint. API Docs: https://github.com/HackerNews/API#new-top-and-best-stories. \u0026#34;\u0026#34;\u0026#34; top_story_ids = requests.get(\u0026#34;https://hacker-news.firebaseio.com/v0/topstories.json\u0026#34;).json() with open(\u0026#34;hackernews_top_story_ids.json\u0026#34;, \u0026#34;w\u0026#34;) as f: json.dump(top_story_ids[:10], f) # asset dependencies can be inferred from parameter names @asset(deps=[hackernews_top_story_ids]) def hackernews_top_stories(context: AssetExecutionContext): \u0026#34;\u0026#34;\u0026#34;Get items based on story ids from the HackerNews items endpoint.\u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;hackernews_top_story_ids.json\u0026#34;, \u0026#34;r\u0026#34;) as f: hackernews_top_story_ids = json.load(f) results = [] for item_id in hackernews_top_story_ids: item = requests.get(f\u0026#34;https://hacker-news.firebaseio.com/v0/item/{item_id}.json\u0026#34;).json() results.append(item) df = pd.DataFrame(results) df.to_csv(\u0026#34;hackernews_top_stories.csv\u0026#34;) # recorded metadata can be customized metadata = { \u0026#34;num_records\u0026#34;: len(df), \u0026#34;preview\u0026#34;: MetadataValue.md(df[[\u0026#34;title\u0026#34;, \u0026#34;by\u0026#34;, \u0026#34;url\u0026#34;]].to_markdown()), } context.add_output_metadata(metadata=metadata) Ở đoạn code trên, chúng ta define 2 function, function đầu tiên là hackernews_top_story_ids có nhiệm vụ fetch data (ID của các story) từ API của Hacker News, sau đó lưu vào file dưới dạng JSON, sau đó function thứ 2 là hackernews_top_stories sẽ đọc dữ liệu từ file JSON vừa lưu, sau đó tiếp tục collect data (dựa trên iD vừa thu được) và lưu vào một Pandas Dataframe. Đoạn code chức năng trong 2 hàm cũng dễ hiểu nên k cần giải thích gì nhỉ :)))\nVà bước quan trọng tiếp theo là thêm decorator @asset cho mỗi function. Và từ đó chúng ta đã có 2 assets lần lượt là hackernews_top_story_ids và hackernews_top_stoies. Ở decorator của function hackernews_top_stories có tham số deps, có nghĩa là asset hackernews_top_stoies phụ thuộc vào asset hackernews_top_story_ids (phải có ID của story mới lấy được story và các thông tin khác). Ô cê cũng dễ nhỉ 😁\nIII. Chạy UI cho trực quan thôi ~~ # Từ từ, hãy đảm bảo bạn đã install cả Pandas nữa :)))\npip install pandas Ô cê rồi, di chuyển đển thư mục chứa hello-dagster.py và chạy lệnh sau:\ndagster dev -f hello-dagster.py Lệnh này sẽ khởi động web server đến host Dagster\u0026rsquo;s UI. Vậy là chúng ta đã thành công khởi động Dagster UI. Mặc định sẽ chạy ở cổng 3000 (có thể đổi cổng bằng option -p khi chạy command). Truy cập vào http://localhost:3000/ và web server sẽ hiển thị.\nVà các bạn đã có thể thấy được UI của Dagster với 2 asset được hiển thị. Mũi tên từ hackernews_top_story_ids đến hackernews_top_stoies Bấm vào nút Materialize All là các bạn có thể chạy thành công pipeline rồi. Data sẽ được collect về và lưu vào file JSON.\nNhưng vẫn còn một thứ khá hay ho, đó là Metadata. Thông tin của asset sẽ được lưu trong metadata. Ở trong hàm hackernews_top_stories ta đã định nghĩa metadata và lưu các thông tin cơ bản của asset trong đó. Ở webserver, ta có thể truy cập và xem thông tin metadata bằng cách click vào asset, sau đó click vào [Show Markdown] ở phần Materialization in Last Run\nVà kết quả sẽ hiển thị như sau:\nLời kết # Ô cê vậy là mình đã hướng dẫn các bạn build và chạy một cái pipeline đơn giản đầu tiên với Dagster. Những bài sau tiếp tục là kiến thức với Dagster nhé. Cảm ơn mọi người đã đọc!\nReference # https://docs.dagster.io/getting-started/hello-dagster\n","date":"2023-08-01","permalink":"/posts/dagster_2/","section":"Posts","summary":"Lời mở đầu # Trong bài trước mình đã giới thiệu với mọi người về Dagster, nó là gì, dùng để làm gì. Trong bài này mình sẽ hướng dẫn các bạn làm một project cơ bản với Dagster, cụ thể là build một data pipeline đơn giản.","title":"Dagster Cơ Bản: Build Một Data Pipeline Đơn Giản Với Dagster"},{"content":"","date":"2023-08-01","permalink":"/tags/data-pipeline/","section":"Tags","summary":"","title":"data pipeline"},{"content":"Day moi la about ne\n","date":"0001-01-01","permalink":"/about/","section":"Hiep's blog","summary":"Day moi la about ne","title":""},{"content":"","date":"0001-01-01","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"0001-01-01","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"0001-01-01","permalink":"/series/","section":"Series","summary":"","title":"Series"}]